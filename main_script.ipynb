{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.layers import LSTM, Embedding, Dropout, Dense\nfrom keras.optimizers import Adam\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom keras.models import load_model\nfrom PIL import Image\nfrom keras import Input\nfrom keras.layers.merge import add\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_dir = 'Flickr8k_text/Flickr8k.token.txt'\n\n###### code to make caption dictionary whose keys are image file name\n###### and values are image caption.\n\n\nimage_captions = open(token_dir).read().split('\\n')\ncaption = {}    \nfor i in range(len(image_captions)-1):\n    id_capt = image_captions[i].split(\"\\t\")\n    id_capt[0] = id_capt[0][:len(id_capt[0])-2] # to rip off the #0,#1,#2,#3,#4 from the tokens file\n    if id_capt[0] in caption:\n        caption[id_capt[0]].append(id_capt[1])\n    else:\n        caption[id_capt[0]] = [id_capt[1]]\n        \n\ncaption['1000268201_693b08cb0e.jpg'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_imgs_id = open(\"Flickr8k_text/Flickr_8k.trainImages.txt\").read().split('\\n')[:-1]\n\n\ntrain_imgs_captions = open(\"Flickr8k_text/trainimgs.txt\",'w')\nfor img_id in train_imgs_id:\n    for captions in caption[img_id]:\n        desc = \"<start> \"+captions+\" <end>\"\n        train_imgs_captions.write(img_id+\"\\t\"+desc+\"\\n\")\n        train_imgs_captions.flush()\ntrain_imgs_captions.close()\n\ntest_imgs_id = open(\"Flickr8k_text/Flickr_8k.testImages.txt\").read().split('\\n')[:-1]\n\ntest_imgs_captions = open(\"Flickr8k_text/testimgs.txt\",'w')\nfor img_id in test_imgs_id:\n    for captions in caption[img_id]:\n        desc = \"<start> \"+captions+\" <end>\"\n        test_imgs_captions.write(img_id+\"\\t\"+desc+\"\\n\")\n        test_imgs_captions.flush()\ntest_imgs_captions.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(x):\n    x /= 255.\n    x -= 0.5\n    x *= 2.\n    return x\n\ndef preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n\n    x = preprocess_input(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = InceptionV3(weights='imagenet')\n\nnew_input = model.input\nnew_output = model.layers[-2].output\nmodel_new = Model(new_input, new_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(image):\n    image = preprocess(image)\n    temp_enc = model_new.predict(image)\n    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n    return temp_enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = 'Flickr8k_Dataset/'\n\ntrain_imgs_id = open(\"Flickr8k_text/Flickr_8k.trainImages.txt\").read().split('\\n')[:-1]\ntest_imgs_id = open(\"Flickr8k_text/Flickr_8k.testImages.txt\").read().split('\\n')[:-1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_train = {}\nfor img in tqdm(train_imgs_id): #tqdm instantly make your loops show a smart progress meter\n    path = images+str(img)\n    encoding_train[img] = encode(path)\n    \nwith open(\"encoded_train_images_inceptionV3.p\", \"wb\") as encoded_pickle: \n    pickle.dump(encoding_train, encoded_pickle) #python object can be pickled so that it can be saved on disk.\n\nencoding_train = pickle.load(open('encoded_train_images_inceptionV3.p', 'rb'))\n\nencoding_train['3556792157_d09d42bef7.jpg'].shape   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding_test = {}\nfor img in tqdm(test_imgs_id):\n    path = images+str(img)\n    encoding_test[img] = encode(path)\n\nwith open(\"encoded_test_images_inceptionV3.p\", \"wb\") as encoded_pickle:\n    pickle.dump(encoding_test, encoded_pickle)\n\nencoding_test = pickle.load(open('encoded_test_images_inceptionV3.p', 'rb')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = pd.read_csv('Flickr8k_text/trainimgs.txt', delimiter='\\t')\ncaptionz = []\nimg_id = []\ndataframe = dataframe.sample(frac=1)\niter = dataframe.iterrows()\n\nfor i in range(len(dataframe)):\n    nextiter = next(iter)\n    captionz.append(nextiter[1][1])\n    img_id.append(nextiter[1][0])\n\nno_samples=0\ntokens = []\ntokens = [i.split() for i in captionz]\nfor caption in captionz:\n    no_samples+=len(caption.split())-1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab= [] \nfor token in tokens:\n    vocab.extend(token)\nvocab = list(set(vocab))\nwith open(\"vocab.p\", \"wb\") as pickle_d:\n   pickle.dump(vocab, pickle_d)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab= pickle.load(open('vocab.p', 'rb'))\nprint (len(vocab))\n\nvocab_size = len(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_idx = {val:index for index, val in enumerate(vocab)}\nidx_word = {index:val for index, val in enumerate(vocab)}\n\nword_idx['end']\n\n\ncaption_length = [len(caption.split()) for caption in captionz]\nmax_length = max(caption_length)\nmax_length # maximum lenght of a caption.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_process(batch_size):\n    partial_captions = []\n    next_words = []\n    images = []\n    total_count = 0\n    while 1:\n    \n        for image_counter, caption in enumerate(captionz):\n            current_image = encoding_train[img_id[image_counter]]\n    \n            for i in range(len(caption.split())-1):\n                total_count+=1\n                partial = [word_idx[txt] for txt in caption.split()[:i+1]]\n                partial_captions.append(partial)\n                next = np.zeros(vocab_size)\n                next[word_idx[caption.split()[i+1]]] = 1\n                next_words.append(next)\n                images.append(current_image)\n\n                if total_count>=batch_size:\n                    next_words = np.asarray(next_words)\n                    images = np.asarray(images)\n                    partial_captions = sequence.pad_sequences(partial_captions, maxlen=max_length, padding='post')\n                    total_count = 0\n                \n                    yield [[images, partial_captions], next_words]\n                    partial_captions = []\n                    next_words = []\n                    images = []\n                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 300\n                \n\ninputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_epochs = 10\nfor i in range(no_epochs): \n    epoch=1\n    batch_size = 128\n    model.fit_generator(data_process(batch_size=batch_size), \n                    steps_per_epoch=no_samples/batch_size,\n                    epochs=epoch, verbose=1, callbacks=None)\n\n\nmodel.save('my_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_captions(image_file):\n    start_word = [\"<start>\"]\n    while 1:\n        now_caps = [word_idx[i] for i in start_word]\n        now_caps = sequence.pad_sequences([now_caps], maxlen=max_length, padding='post')\n        e = encode(image_file)\n        preds = fin_model.predict([np.array([e]), np.array(now_caps)])\n        word_pred = idx_word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_length: \n    #keep on predicting next word unitil word predicted is <end> or caption lenghts is greater than max_lenght(40)\n            break\n            \n    return ' '.join(start_word[1:-1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_file =\"horse.png\"\nImage.open(image_file)\nprint ('Normal Max search:', predict_captions(image_file)) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}